{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5DwxYIRavMST"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SHl0bkPCvayd"
   },
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"your API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BV1o0PmcvyJF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have a really smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  At first, the puppy doesn't know what those words mean, but you show it what to do, and it learns.  It might make mistakes at first, but with practice, it gets better and better.\n",
      "\n",
      "Artificial Intelligence, or AI, is like that smart puppy, but instead of learning tricks, it learns from information.  We give the computer lots and lots of information – pictures, words, numbers – and it learns patterns and how to do things based on that information.\n",
      "\n",
      "For example, we could show it lots of pictures of cats and dogs, and it would learn to tell the difference between them. Or we could show it lots of examples of sentences, and it would learn to write its own sentences.\n",
      "\n",
      "It's not actually *thinking* like you and me, but it's really good at following instructions and finding patterns in information. It's like a super-powered puppy that can learn incredibly fast!  We use AI for lots of cool things, like helping doctors make diagnoses, recommending movies you might like, and even driving self-driving cars.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = flash.generate_content(\"Explain AI to me like I'm a kid.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f60ed9d8ae41"
   },
   "source": [
    "The response often comes back in markdown format, which you can render directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c933e5e460a5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a really smart puppy.  You teach it tricks, like \"sit\" and \"fetch.\"  At first, the puppy doesn't know what those words mean, but you show it what to do, and it learns.  It might make mistakes at first, but with practice, it gets better and better.\n",
       "\n",
       "Artificial Intelligence, or AI, is like that smart puppy, but instead of learning tricks, it learns from information.  We give the computer lots and lots of information – pictures, words, numbers – and it learns patterns and how to do things based on that information.\n",
       "\n",
       "For example, we could show it lots of pictures of cats and dogs, and it would learn to tell the difference between them. Or we could show it lots of examples of sentences, and it would learn to write its own sentences.\n",
       "\n",
       "It's not actually *thinking* like you and me, but it's really good at following instructions and finding patterns in information. It's like a super-powered puppy that can learn incredibly fast!  We use AI for lots of cool things, like helping doctors make diagnoses, recommending movies you might like, and even driving self-driving cars.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lV_S5ZL5MidD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a pleasure to meet you, Dr. Ganapathi Raju.  How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = flash.start_chat(history=[])\n",
    "response = chat.send_message('Hello! My name is Dr Ganapathi Raju.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7b0372c3c64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most interesting things about Generative AI is its capacity for **emergent behavior**.  This means that the AI can produce outputs that weren't explicitly programmed or even anticipated by its creators.  For example, a large language model might be trained on vast amounts of text data to predict the next word in a sequence.  However, through the sheer scale and complexity of its training, it can spontaneously develop abilities like summarizing complex topics, translating languages, or even writing creative text formats like poems or code – skills not directly taught, but emergent properties of the underlying architecture and training data.  This emergent behavior makes Generative AI both incredibly powerful and somewhat unpredictable, leading to ongoing research into understanding and controlling these unexpected capabilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell something interesting about Gen AI?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "d3f9591392a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you told me your name is Dr. Ganapathi Raju.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# While you have the `chat` object around, the conversation state\n",
    "# persists. Confirm that by asking if it knows my name.\n",
    "response = chat.send_message('Do you remember what my name is?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KAjpr1200sW"
   },
   "source": [
    "### Choose a model\n",
    "\n",
    "The Gemini API provides access to a number of models from the Gemini model family. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uUUZa2uq2jDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemini-exp-1114\n",
      "models/gemini-exp-1121\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN49kSI54R1v"
   },
   "source": [
    "The [`models.list`](https://ai.google.dev/api/models#method:-models.list) response also returns additional information about the model's capabilities, like the token limits and supported parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "k7JJ1K6j4Rl8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  if model.name == 'models/gemini-1.5-flash':\n",
    "    print(model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rU_UBlZdooM"
   },
   "source": [
    "## Explore generation parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7NfEizeipbW"
   },
   "source": [
    "### Output length\n",
    "\n",
    "When generating text with an LLM, the output length affects cost and performance. \n",
    "\n",
    "Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n",
    "\n",
    "To stop the model from generating tokens past a limit, you can specify the `max_output_tokens` parameter when using the Gemini API. \n",
    "\n",
    "Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached. \n",
    "\n",
    "Prompt engineering may be required to generate a more complete output for your given limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qVf23JsIi9ma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Profound Impact of Generative AI on Modern Society\n",
      "\n",
      "Generative artificial intelligence (Gen AI), encompassing models capable of creating new content such as text, images, audio, and video, is rapidly transforming modern society. Its impact extends far beyond the realm of technological advancement, profoundly influencing various sectors and prompting crucial ethical considerations.  While still nascent, its potential to reshape our world is undeniable, presenting both immense opportunities and significant challenges. This essay will explore the multifaceted importance of Gen AI in contemporary society, examining its applications, implications, and the crucial need for responsible development and deployment.\n",
      "\n",
      "One of the most significant contributions of Gen AI is its capacity to enhance productivity and efficiency across diverse industries. In the creative sector, Gen AI tools are streamlining workflows.  Writers leverage AI to overcome writer's block, generating ideas and refining drafts.  Graphic designers utilize AI-powered tools to produce unique visuals and accelerate the design process.  Musicians can use AI to compose melodies and generate harmonies,\n"
     ]
    }
   ],
   "source": [
    "short_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
    "\n",
    "response = short_model.generate_content('Write a 1000 word essay on the importance of Gen AI modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "W-3kR2F5kdMR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vast oceans of data, a swirling, deep source,\n",
      "LLMs unlock knowledge, charting a new course.\n",
      "From text to translation, creation's swift art,\n",
      "A helping hand, a brand new, digital heart.\n",
      "Though flaws may exist, and biases reside,\n",
      "Their potential immense, a future we can ride.\n",
      "With careful guidance, and ethical hand,\n",
      "LLMs will reshape our world, across the land.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = short_model.generate_content('Write a short poem on the importance of LLMs.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZhDSLB6lqqB"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alx-WaAvir_9"
   },
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature controls the degree of randomness in token selection.\n",
    "\n",
    "Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
    "\n",
    "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat.\n",
    "\n",
    "**Note that if you see a 429 Resource Exhausted error here, you may be able to tweak the prompt slightly to progress.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SHraGMzqnZqt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marigold\n",
      " -------------------------\n",
      "Marigold\n",
      " -------------------------\n",
      "Purple\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Marigold\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code \n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)', request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3J4pCTuof7e"
   },
   "source": [
    "Now try the same prompt with temperature set to zero. \n",
    "\n",
    "Note that the output is not completely deterministic, as other parameters affect token selection, but the results will tend to be more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "clymkWv-PfUZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n",
      "Maroon\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "for _ in range(5):\n",
    "  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                             request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=aDmp2Uim0zQ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "St5nt3vzitsZ"
   },
   "source": [
    "### Top-K and top-P\n",
    "\n",
    "Like temperature, top-K and top-P parameters are also used to control the diversity of the model's output.\n",
    "\n",
    "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. \n",
    "\n",
    "A top-K of 1 selects a single token, performing greedy decoding.\n",
    "\n",
    "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. \n",
    "\n",
    "A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
    "\n",
    "When both are supplied, the Gemini API will filter top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature.\n",
    "\n",
    "Run this example a number of times, change the settings and observe the change in output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lPlzpEavUV8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartholomew, a ginger tabby with a penchant for trouble, wasn't your average house cat. He dreamt of the world beyond the window, the world of rustling leaves and chirping birds, a world he only glimpsed through frosted panes. One day, an open window became his ticket to freedom. He squeezed through, landing with a soft thud on the dew-kissed grass.\n",
      "\n",
      "The world was a symphony of smells – damp earth, blooming honeysuckle, and the musky scent of a nearby fox. Bartholomew, his tail twitching with excitement, embarked on his adventure. He stalked a plump robin, narrowly missing its escape. He climbed a towering oak, its branches offering breathtaking views of the sprawling countryside. The sun dipped below the horizon, painting the sky in hues of orange and purple, and Bartholomew, nestled in the crook of a tree, felt a profound sense of peace.\n",
      "\n",
      "He stumbled upon a village, its houses lit by warm, welcoming glows. He met a dog, a lumbering golden retriever, who eyed him curiously. He spent the night in a cozy shed, the scent of hay and sawdust comforting him. The next day, he followed the babbling brook, its water sparkling in the sun, until it led him to a bustling town. He marveled at the hustle and bustle, the honking horns, and the smell of fresh bread wafting from a bakery.\n",
      "\n",
      "But amidst the chaos, Bartholomew felt a pang of longing for the familiar scent of his home. He missed the warmth of his cat bed, the soft purrs of his human companion. The adventure, exhilarating as it was, felt incomplete.\n",
      "\n",
      "He retraced his steps, guided by the faint scent of home. He found the open window and squeezed back inside, his heart pounding with exhaustion and contentment. He curled up on his familiar bed, the world outside a distant memory. He was home, and that, he realized, was the greatest adventure of all.\n",
      "\n",
      "From then on, Bartholomew would sit by the window, his gaze fixed on the world beyond. But the world he truly cherished was the one inside, a world of love, comfort, and the quiet hum of a contented heart. He had tasted freedom, but he had also discovered that the greatest adventures often began and ended within the familiar walls of home. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        # These are the default values for gemini-1.5-flash-001.\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMrYs1koY6DX"
   },
   "source": [
    "## Prompting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhj_tQidZJP7"
   },
   "source": [
    "### Zero-shot\n",
    "\n",
    "Zero-shot prompts are prompts that describe the request for the model directly.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "1_t-cwnDZzbH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **POSITIVE**\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5,\n",
    "    ))\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b5568bdeb11"
   },
   "source": [
    "#### Enum mode\n",
    "\n",
    "The models are trained to generate text, and can sometimes produce more text than you may wish for. \n",
    "\n",
    "In the preceding example, the model will output the label, sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards.\n",
    "\n",
    "The Gemini API has an [Enum mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) feature that allows you to constrain the output to a fixed set of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ad118a56c598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0udiSwNbv45W"
   },
   "source": [
    "### One-shot and few-shot\n",
    "\n",
    "Providing an example of the expected response is known as a \"one-shot\" prompt.\n",
    "\n",
    "When you provide multiple examples, it is a \"few-shot\" prompt.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hd4mVUukwOKZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"size\": \"large\",\n",
      "  \"type\": \"normal\",\n",
      "  \"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ))\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "021293096f08"
   },
   "source": [
    "#### JSON mode\n",
    "\n",
    "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's [JSON mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb). \n",
    "\n",
    "This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "50fbf0260912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert pizza\"}\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a93e338e57c"
   },
   "source": [
    "### Chain of Thought (CoT)\n",
    "\n",
    "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. \n",
    "\n",
    "The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
    "\n",
    "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. \n",
    "\n",
    "It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
    "\n",
    "As models like the Gemini family are trained to be \"chatty\" and provide reasoning steps, you can ask the model to be more direct in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5715555db1c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 * 3 = 12\n",
      "\n",
      "20 - 4 = 16  (difference in ages)\n",
      "\n",
      "12 + 16 = 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e12b19677bfd"
   },
   "source": [
    "Now try the same approach, but indicate to the model that it should \"think step by step\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ffd7536a481f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve this step-by-step:\n",
      "\n",
      "1. **Partner's age when you were 4:** When you were 4, your partner was 3 times your age, meaning they were 3 * 4 = 12 years old.\n",
      "\n",
      "2. **Age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n",
      "\n",
      "3. **Partner's current age:** Since you are now 20, and the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPiZ_eIIaVPt"
   },
   "source": [
    "## Code prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZinKamwXeR6C"
   },
   "source": [
    "### Generating code\n",
    "\n",
    "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n",
    "\n",
    "It's important to be aware that since LLMs can't reason, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1YX71JGtzDjXQkgdes8bP6i3oH5lCRKxv\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fOQP9pqmeUO1"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ))\n",
    "\n",
    "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlBMWSFhgVRQ"
   },
   "source": [
    "### Code execution\n",
    "\n",
    "The Gemini API can automatically run generated code too, and will return the output.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jT3OfWYfhjRL"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To calculate the sum of the first 14 odd prime numbers, I will first generate the first 14 odd prime numbers and then compute their sum.  I will use Python for this task.\n",
       "\n",
       "\n",
       "``` python\n",
       "import sympy\n",
       "\n",
       "primes = []\n",
       "count = 0\n",
       "num = 3\n",
       "while count < 14:\n",
       "    if sympy.isprime(num):\n",
       "        primes.append(num)\n",
       "        count += 1\n",
       "    num += 2\n",
       "\n",
       "print(f'{primes=}')\n",
       "print(f'Sum of the first 14 odd primes: {sum(primes)}')\n",
       "\n",
       "```\n",
       "```\n",
       "primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "Sum of the first 14 odd primes: 326\n",
       "\n",
       "```\n",
       "The code generates the first 14 odd prime numbers and then calculates their sum.  The output shows that the first 14 odd prime numbers are [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47] and their sum is 326.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution',)\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZspT1GSkjG6d"
   },
   "source": [
    "While this looks like a single-part response, you can inspect the response to see the each of the steps: initial text, code generation, execution results, and final text summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "j4gQVzcRjRX-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"To calculate the sum of the first 14 odd prime numbers, I will first generate the first 14 odd prime numbers and then compute their sum.  I will use Python for this task.\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\nimport sympy\\n\\nprimes = []\\ncount = 0\\nnum = 3\\nwhile count < 14:\\n    if sympy.isprime(num):\\n        primes.append(num)\\n        count += 1\\n    num += 2\\n\\nprint(f\\'{primes=}\\')\\nprint(f\\'Sum of the first 14 odd primes: {sum(primes)}\\')\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "  output: \"primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nSum of the first 14 odd primes: 326\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"The code generates the first 14 odd prime numbers and then calculates their sum.  The output shows that the first 14 odd prime numbers are [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47] and their sum is 326.\\n\"\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "  print(part)\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gUX8QzCj4d5"
   },
   "source": [
    "### Explaining code\n",
    "\n",
    "The Gemini family of models can explain code to you too.\n",
    "\n",
    "<table align=left>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1N7LGzWzCYieyOf_7bAG4plrmkpDNmUyb\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7_jPMMoxkIEb"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file is a bash script that enhances your shell prompt to display information about your current Git repository.  It's essentially a highly configurable Git prompt.\n",
       "\n",
       "You would use it to:\n",
       "\n",
       "* **Improve your workflow:** By seeing your branch, status (e.g., changes, untracked files), and other Git information directly in your shell prompt, you gain quick awareness of your repository's state without having to explicitly run `git status`.\n",
       "* **Customize your prompt:**  The script is heavily customizable. You can choose different themes (color schemes), select what information to show (e.g., only branch name, or also upstream changes), and adjust the formatting.  You can even create your own custom theme.\n",
       "* **Support various shells:** It aims for compatibility with both Bash and Zsh.\n",
       "\n",
       "In short, it's a tool that integrates Git status into your shell prompt for improved productivity and a more visually appealing command line experience.  The many functions handle the logic of fetching Git information, applying themes and user settings, and updating the prompt accordingly.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.google.dev/gemini-api/prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.google.dev/gemini-api/docs/prompting-intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-1-prompting.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
